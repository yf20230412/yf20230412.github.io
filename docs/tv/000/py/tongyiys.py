# -*- coding: utf-8 -*-
# ç»Ÿä¸€å½±è§†çˆ¬è™« - https://tyys2.com/ (ç²¾ç®€ç‰ˆ)
# ä½œè€…: AI Assistant
# è¯´æ˜: åŸºäºPyramidStoreæ¡†æ¶çš„å½±è§†çˆ¬è™«ï¼Œç²¾ç®€ä¼˜åŒ–ç‰ˆæœ¬

import sys
import re
import json
import urllib.parse
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.append(parent_dir)

from base.spider import Spider


class Spider(Spider):

    def init(self, extend=""):
        self.host = 'https://tyys2.com'
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }

    def getName(self):
        return "ç»Ÿä¸€å½±è§†"

    def _extract_video_info(self, element):
        """æå–è§†é¢‘ä¿¡æ¯"""
        try:
            # è·å–è§†é¢‘é“¾æ¥å’ŒID
            link_elem = element.xpath('.//a[@class="public-list-exp"]/@href')
            if not link_elem:
                return None

            link = link_elem[0]
            vod_id = self.regStr(r'/detail/id/(\d+)\.html', link)
            if not vod_id:
                return None

            # è·å–æ ‡é¢˜
            title_selectors = [
                './/a[@class="time-title"]/text()',
                './/a[contains(@class,"time-title")]/text()',
                './/a[@class="public-list-exp"]/@title'
            ]

            title = ''
            for title_selector in title_selectors:
                title_elem = element.xpath(title_selector)
                if title_elem:
                    title = title_elem[0].strip()
                    break

            # è·å–å°é¢å›¾ç‰‡
            pic_elem = element.xpath('.//img/@data-src | .//img/@src')
            pic = ''
            for pic_url in pic_elem:
                if pic_url and not pic_url.startswith('data:'):
                    pic = pic_url
                    if not pic.startswith('http'):
                        pic = self.host + pic
                    break

            # è·å–æ›´æ–°çŠ¶æ€
            remarks_selectors = [
                './/span[@class="public-list-prb"]/text()',
                './/span[contains(@class,"public-list-prb")]/text()',
                './/div[@class="public-list-subtitle"]/text()'
            ]

            remarks = ''
            for remarks_selector in remarks_selectors:
                remarks_elem = element.xpath(remarks_selector)
                if remarks_elem:
                    remarks = remarks_elem[0].strip()
                    break

            if title and vod_id:
                return {
                    'vod_id': vod_id,
                    'vod_name': title,
                    'vod_pic': pic,
                    'vod_remarks': remarks,
                    'vod_year': ''
                }
        except Exception as e:
            self.log(f"è§£æè§†é¢‘ä¿¡æ¯å‡ºé”™: {str(e)}")
        return None

    def homeContent(self, filter):
        """è·å–é¦–é¡µå†…å®¹å’Œåˆ†ç±»"""
        result = {}
        
        # å®šä¹‰åˆ†ç±»
        classes = [
            {'type_name': 'ç”µè§†å‰§', 'type_id': '1'},
            {'type_name': 'ç”µå½±', 'type_id': '2'},
            {'type_name': 'åŠ¨æ¼«', 'type_id': '3'},
            {'type_name': 'ç»¼è‰º', 'type_id': '4'},
            {'type_name': 'ä½“è‚²èµ›äº‹', 'type_id': '5'},
            {'type_name': 'çŸ­å‰§', 'type_id': '41'},
        ]
        result['class'] = classes
        
        # è·å–é¦–é¡µæ¨èå†…å®¹
        try:
            rsp = self.fetch(self.host, headers=self.headers)
            content = rsp.text
            doc = self.html(content)

            # æå–æ¨èè§†é¢‘
            video_elements = doc.xpath('//div[contains(@class,"public-list-box")]')
            videos = []
            for element in video_elements:
                video_info = self._extract_video_info(element)
                if video_info:
                    videos.append(video_info)
            
            result['list'] = videos
            
        except Exception as e:
            self.log(f"è·å–é¦–é¡µå†…å®¹å‡ºé”™: {str(e)}")
            result['list'] = []
        
        return result

    def categoryContent(self, tid, pg, filter, extend):
        """è·å–åˆ†ç±»å†…å®¹"""
        result = {}
        videos = []
        
        try:
            # æ„å»ºåˆ†ç±»é¡µé¢URL
            if pg == '1':
                url = f"{self.host}/index.php/vod/type/id/{tid}.html"
            else:
                url = f"{self.host}/index.php/vod/type/id/{tid}/page/{pg}.html"
            
            rsp = self.fetch(url, headers=self.headers)
            content = rsp.text
            doc = self.html(content)
            
            # è§£æè§†é¢‘åˆ—è¡¨
            video_elements = doc.xpath('//div[contains(@class,"public-list-box")]')
            for element in video_elements:
                video_info = self._extract_video_info(element)
                if video_info:
                    videos.append(video_info)
            
            result['list'] = videos
            result['page'] = pg
            result['pagecount'] = 999
            result['limit'] = 20
            result['total'] = 999 * 20
            
        except Exception as e:
            self.log(f"è·å–åˆ†ç±»å†…å®¹å‡ºé”™: {str(e)}")
            result['list'] = []
            result['page'] = pg
            result['pagecount'] = 1
            result['limit'] = 20
            result['total'] = 0
        
        return result

    def _parse_play_sources(self, doc):
        """è§£ææ’­æ”¾æº - ç²¾ç®€ç‰ˆ"""
        play_from = []
        play_url = []
        
        # æŸ¥æ‰¾æ‰€æœ‰æ’­æ”¾é“¾æ¥ï¼ŒæŒ‰sidåˆ†ç»„
        all_episodes = doc.xpath('//a[contains(@href,"/vod/play/")]')
        sid_groups = {}
        
        for ep in all_episodes:
            ep_url = ep.xpath('./@href')[0] if ep.xpath('./@href') else ''
            ep_title = ep.xpath('./text()')[0] if ep.xpath('./text()') else ''
            
            if ep_url:
                sid_match = re.search(r'/sid/(\d+)/', ep_url)
                if sid_match:
                    sid = sid_match.group(1)
                    if sid not in sid_groups:
                        sid_groups[sid] = []
                    sid_groups[sid].append((ep_title, ep_url))
        
        # æ’­æ”¾æºåç§°æ˜ å°„
        sid_names = {
            '9': 'è¶…æ¸…', '6': 'è“å…‰5', '7': 'å¤‡ç”¨1', '4': 'å¤‡ç”¨2',
            '8': 'å¤‡ç”¨3', '1': 'å¤‡ç”¨4', '2': 'å¤‡ç”¨5', '3': '1080', 
            '5': 'æ’­æ”¾æº5', '10': 'æ’­æ”¾æº10', '11': 'æ’­æ”¾æº11', '12': 'æ’­æ”¾æº12'
        }
        
        # å¿«é€Ÿè´¨é‡æ’åº
        def get_priority(sid, source_name):
            if sid == '3' or '1080' in source_name.lower():
                return 1  # 1080Pæœ€é«˜ä¼˜å…ˆçº§
            elif sid == '9':
                return 2  # è¶…æ¸…
            elif sid == '6':
                return 3  # è“å…‰5
            elif sid == '7':
                return 4  # å¤‡ç”¨1
            elif sid == '4':
                return 5  # å¤‡ç”¨2
            elif sid == '5':
                return 6  # æ’­æ”¾æº5
            elif sid == '1':
                return 7  # å¤‡ç”¨4
            elif sid == '2':
                return 8  # å¤‡ç”¨5
            else:
                return 9  # å…¶ä»–
        
        # æ’åºå¹¶ç”Ÿæˆæ’­æ”¾æº
        sorted_sids = sorted(sid_groups.keys(), 
                           key=lambda x: get_priority(x, sid_names.get(x, f'æ’­æ”¾æº{x}')))
        
        used_names = set()
        for sid in sorted_sids:
            episodes = sid_groups[sid]
            if episodes:
                source_name = sid_names.get(sid, f'æ’­æ”¾æº{sid}')
                
                # æ™ºèƒ½é‡å‘½å
                if sid == '3':
                    base_name = "1080"
                elif sid == '9':
                    base_name = "è¶…æ¸…"
                elif sid == '6':
                    base_name = "è“å…‰5"
                else:
                    base_name = source_name
                
                # ç¡®ä¿å”¯ä¸€æ€§
                final_name = base_name
                counter = 1
                while final_name in used_names:
                    final_name = f"{base_name}_{counter}"
                    counter += 1
                used_names.add(final_name)
                
                # æ·»åŠ SIDæ ‡è¯†ç¡®ä¿æ’­æ”¾æ—¶æ­£ç¡®è¯†åˆ«
                display_name = f"{final_name}[å°é±¼ğŸ¬{sid}]"
                play_from.append(display_name)
                
                episode_list = []
                for ep_title, ep_url in episodes:
                    if ep_title and ep_url:
                        episode_list.append(f"{ep_title}${ep_url}")
                
                play_url.append('#'.join(episode_list))
        
        return play_from, play_url

    def searchContent(self, key, quick, pg="1"):
        """æœç´¢å†…å®¹ - ä¿®å¤ç‰ˆ"""
        result = {}
        videos = []

        try:
            # æ„å»ºæœç´¢URLï¼ˆä½¿ç”¨GETè¯·æ±‚ï¼‰
            search_url = f"{self.host}/index.php/vod/search.html"
            params = {'wd': key}
            if pg != '1':
                params['page'] = pg

            url = f"{search_url}?{urllib.parse.urlencode(params)}"
            self.log(f"æœç´¢URL: {url}")
            rsp = self.fetch(url, headers=self.headers)
            doc = self.html(rsp.text)

            # æŸ¥æ‰¾æœç´¢ç»“æœå®¹å™¨
            search_containers = doc.xpath('//div[contains(@class,"search-item") or contains(@class,"module-search-item") or contains(@class,"public-list-box")]')

            if not search_containers:
                search_containers = doc.xpath('//a[contains(@href,"/detail/")]/ancestor::*[position()<=3]')

            self.log(f"æ‰¾åˆ° {len(search_containers)} ä¸ªæœç´¢å®¹å™¨")

            processed_ids = set()

            for container in search_containers:
                detail_links = container.xpath('.//a[contains(@href,"/detail/")]')

                for link in detail_links:
                    href = link.xpath('./@href')
                    if href:
                        vod_id = self.regStr(r'/detail/id/(\d+)\.html', href[0])
                        if vod_id and vod_id not in processed_ids:
                            processed_ids.add(vod_id)

                            # æå–æ ‡é¢˜
                            title = self._extract_search_title(container, link)

                            # è¿‡æ»¤æ— å…³ç»“æœ
                            if title and vod_id and title != 'æ’­æ”¾æ­£ç‰‡' and self._is_relevant_search_result(title, key):
                                # è·å–å›¾ç‰‡ã€å¤‡æ³¨ç­‰ä¿¡æ¯
                                pic, remarks, year, area, type_name = self._get_search_info(container)

                                videos.append({
                                    'vod_id': vod_id,
                                    'vod_name': title,
                                    'vod_pic': pic,
                                    'vod_remarks': remarks,
                                    'vod_year': year,
                                    'vod_area': area,
                                    'vod_tag': type_name
                                })
                                self.log(f"âœ… ç›¸å…³è§†é¢‘: {title} (ID: {vod_id})")
                                break
                            elif title:
                                self.log(f"âŒ è¿‡æ»¤æ— å…³: {title} (æœç´¢: {key})")

            result['list'] = videos
            self.log(f"æœ€ç»ˆæœç´¢ç»“æœ: {len(videos)} ä¸ªè§†é¢‘")

        except Exception as e:
            self.log(f"æœç´¢å‡ºé”™: {str(e)}")
            result['list'] = []

        return result

    def _is_relevant_search_result(self, title, search_key):
        """æ£€æŸ¥æœç´¢ç»“æœæ˜¯å¦ç›¸å…³"""
        if not title or not search_key:
            return False

        title_lower = title.lower()
        search_key_lower = search_key.lower()

        # ç›´æ¥åŒ…å«æœç´¢å…³é”®è¯
        if search_key_lower in title_lower:
            return True

        # è¿‡æ»¤æ˜æ˜¾ä¸ç›¸å…³çš„å†…å®¹
        irrelevant_keywords = ['å“ˆå“ˆå“ˆå“ˆå“ˆ', 'æ‰§æ³•è€…ä»¬', 'åˆ‘ä¾¦', 'éŸ¶åè‹¥é”¦', 'é™·å…¥æˆ‘ä»¬çš„çƒ­æ‹', 'æŠ¤å®å¯»è¸ª', 'é€†å¤©æˆä»™', 'æŠ˜è…°', 'è—æµ·ä¼ ', 'ä¸´æ±Ÿä»™', 'çŸ³æ¥è¿è½¬', 'å¤§ä¾¦æ¢', 'é•¿å®‰çš„è”æ', 'æ¿‘æˆ·éº»æ²™ç¾', 'ä¼Šåˆ©å¨…', 'æ›¾ç»æ½‡æ´’', 'æŸå¤©ï¼Œè‰å¸½', 'å¨œç¾å¶ç„¶', 'ä¼ å¥‡æµ·ç›—']

        for irrelevant in irrelevant_keywords:
            if irrelevant in title:
                return False

        # ç‰¹å®šä½œå“çš„ä¸¥æ ¼åŒ¹é…
        specific_works = {
            'æ–—ç½—å¤§é™†': ['æ–—ç½—', 'å”é—¨', 'ç»ä¸–', 'é¾™ç‹', 'ç‡ƒé­‚'],
            'å‡¡äººä¿®ä»™ä¼ ': ['å‡¡äºº', 'ä¿®ä»™', 'éŸ©ç«‹'],
            'æµ·è´¼ç‹': ['æµ·è´¼', 'èˆªæµ·ç‹', 'è·¯é£', 'one piece'],
            'ç«å½±å¿è€…': ['ç«å½±', 'å¿è€…', 'é¸£äºº'],
            'æ­»ç¥': ['æ­»ç¥', 'ä¸€æŠ¤', 'bleach'],
            'è¿›å‡»çš„å·¨äºº': ['è¿›å‡»', 'å·¨äºº', 'è‰¾ä¼¦'],
            'é¬¼ç­ä¹‹åˆƒ': ['é¬¼ç­', 'ç‚­æ²»éƒ', 'é¬¼æ€é˜Ÿ'],
            'é¾™ç ': ['é¾™ç ', 'æ‚Ÿç©º', 'èµ›äºšäºº']
        }

        for work, keywords in specific_works.items():
            if work in search_key_lower:
                return any(keyword in title_lower for keyword in keywords)

        # å­—ç¬¦åŒ¹é…ï¼ˆé«˜é˜ˆå€¼ï¼‰
        search_chars = set(search_key_lower.replace(' ', ''))
        title_chars = set(title_lower.replace(' ', ''))

        if len(search_chars) > 0:
            match_ratio = len(search_chars & title_chars) / len(search_chars)
            if match_ratio >= 0.9:
                return True

        # æœç´¢è¯å˜ä½“
        search_variants = {
            'æ–—ç½—': ['æ–—ç½—', 'å”é—¨', 'ç»ä¸–'],
            'å‡¡äºº': ['å‡¡äºº', 'ä¿®ä»™'],
            'æµ·è´¼': ['æµ·è´¼', 'èˆªæµ·ç‹'],
            'ç«å½±': ['ç«å½±', 'å¿è€…'],
            'é¾™ç ': ['é¾™ç ', 'æ‚Ÿç©º']
        }

        for base_word, variants in search_variants.items():
            if base_word in search_key_lower:
                return any(variant in title_lower for variant in variants)

        # çŸ­æœç´¢è¯è¦æ±‚ä¸¥æ ¼åŒ¹é…
        if len(search_key_lower) <= 2:
            return search_key_lower in title_lower

        return False

    def _extract_search_title(self, container, link):
        """æå–æœç´¢ç»“æœæ ‡é¢˜"""
        title = ''

        try:
            # ä»é“¾æ¥titleå±æ€§è·å–
            title_attr = link.xpath('./@title')
            if title_attr and title_attr[0].strip():
                title = title_attr[0].strip()
                if title and title not in ['æ’­æ”¾æ­£ç‰‡', 'è¯¦æƒ…', 'è§‚çœ‹'] and len(title) < 80:
                    return title

            # ä»å®¹å™¨æ–‡æœ¬ä¸­æŸ¥æ‰¾
            all_texts = container.xpath('.//text()')
            candidate_titles = []

            for text in all_texts:
                text = text.strip()
                if (text and 2 < len(text) < 50 and
                    text not in ['æ’­æ”¾æ­£ç‰‡', 'è¯¦æƒ…', 'è§‚çœ‹', 'å·²å®Œç»“', 'æ›´æ–°ä¸­', 'HD', 'TC', 'TS', 'BD', 'NO'] and
                    'æ›´æ–°è‡³' not in text and 'å¯¼æ¼”ï¼š' not in text and 'ä¸»æ¼”ï¼š' not in text and
                    'è¯¥å‰§æ”¹ç¼–' not in text and 'ä¼ å¥‡æµ·ç›—' not in text and
                    not text.isdigit() and '/' not in text and 'å¹´' not in text and
                    text not in ['å¤§é™†', 'ä¸­å›½å¤§é™†', 'ç¾å›½', 'æ—¥æœ¬', 'éŸ©å›½', 'è‹±å›½', 'æ³•å›½'] and
                    text not in ['å›½äº§åŠ¨æ¼«', 'æ—¥éŸ©åŠ¨æ¼«', 'æ¬§ç¾åŠ¨æ¼«', 'åŠ¨æ¼«ç”µå½±', 'ç”µè§†å‰§', 'ç”µå½±', 'ç»¼è‰º', 'ä½“è‚²èµ›äº‹'] and
                    'åŠ¨æ¼«' not in text and 'ç”µå½±' not in text and 'ç”µè§†å‰§' not in text and
                    not re.match(r'^\d{4}$', text) and not re.match(r'^NO\s*\d+$', text) and
                    'ï¼Œ' not in text and 'ã€‚' not in text):
                    candidate_titles.append(text)

            if candidate_titles:
                # ä¼˜å…ˆé€‰æ‹©åŒ…å«å…³é”®è¯çš„æ ‡é¢˜
                for candidate in candidate_titles:
                    if any(keyword in candidate for keyword in ['æ–—ç½—', 'ä¼ è¯´', 'ç»ä¸–', 'é¾™ç‹', 'ç‡ƒé­‚', 'å‡¡äºº', 'ä¿®ä»™', 'æµ·è´¼ç‹', 'èˆªæµ·ç‹']):
                        title = candidate
                        break

                if not title:
                    suitable_titles = [t for t in candidate_titles if 3 <= len(t) <= 30]
                    if suitable_titles:
                        title = suitable_titles[0]

            # ä»é“¾æ¥æ–‡æœ¬è·å–
            if not title or title in ['æ’­æ”¾æ­£ç‰‡', 'è¯¦æƒ…']:
                link_text = link.xpath('./text()')
                if link_text and link_text[0].strip():
                    text = link_text[0].strip()
                    if text not in ['æ’­æ”¾æ­£ç‰‡', 'è¯¦æƒ…', 'è§‚çœ‹'] and len(text) < 50:
                        title = text

            # æ¸…ç†æ ‡é¢˜
            if title:
                title = re.sub(r'^(NO\s*\d+\s*)', '', title).strip()
                if len(title) > 50:
                    title = ''

        except Exception as e:
            self.log(f"æå–æœç´¢æ ‡é¢˜å‡ºé”™: {str(e)}")

        return title if title else ''

    def _get_search_info(self, container):
        """è·å–æœç´¢ç»“æœçš„å›¾ç‰‡ã€å¤‡æ³¨ã€å…ƒä¿¡æ¯"""
        pic = ''
        remarks = ''
        year = ''
        area = ''
        type_name = ''

        try:
            # è·å–å›¾ç‰‡
            pic_elements = container.xpath('.//img/@data-src | .//img/@src')
            for pic_url in pic_elements:
                if pic_url and not pic_url.startswith('data:'):
                    pic = pic_url if pic_url.startswith('http') else self.host + pic_url
                    break

            # è·å–æ‰€æœ‰æ–‡æœ¬ç”¨äºæå–ä¿¡æ¯
            all_texts = container.xpath('.//text()')
            all_info_text = ' '.join([text.strip() for text in all_texts if text.strip()])

            # è·å–å¤‡æ³¨
            for text in all_texts:
                text = text.strip()
                if ('æ›´æ–°' in text or 'å®Œç»“' in text or 'é›†' in text) and len(text) < 50:
                    remarks = text
                    break

            # æå–å¹´ä»½ã€åœ°åŒºã€ç±»å‹
            year_match = re.search(r'(\d{4})', all_info_text)
            if year_match:
                year = year_match.group(1)

            area_match = re.search(r'(å¤§é™†|é¦™æ¸¯|å°æ¹¾|ç¾å›½|æ—¥æœ¬|éŸ©å›½|è‹±å›½|æ³•å›½|å¾·å›½|æ„å¤§åˆ©|åŠ æ‹¿å¤§|æ¾³å¤§åˆ©äºš|æ³°å›½|å°åº¦|ä¸­å›½å¤§é™†)', all_info_text)
            if area_match:
                area = area_match.group(1)

            type_match = re.search(r'(å›½äº§åŠ¨æ¼«|æ—¥éŸ©åŠ¨æ¼«|æ¬§ç¾åŠ¨æ¼«|åŠ¨æ¼«ç”µå½±|å‰§æƒ…|å–œå‰§|åŠ¨ä½œ|çˆ±æƒ…|ç§‘å¹»|åŠ¨æ¼«|æ‚¬ç–‘|ææ€–|æˆ˜äº‰|å†å²|çŠ¯ç½ª|å¥‡å¹»|é’æ˜¥|å¤è£…|å†’é™©|æ­¦ä¾ |å¶åƒ)', all_info_text)
            if type_match:
                type_name = type_match.group(1)

        except Exception as e:
            self.log(f"æå–æœç´¢ä¿¡æ¯å‡ºé”™: {str(e)}")

        return pic, remarks, year, area, type_name

    def detailContent(self, ids):
        """è·å–è¯¦æƒ…å†…å®¹"""
        result = {}
        videos = []

        try:
            for vod_id in ids:
                detail_url = f"{self.host}/index.php/vod/detail/id/{vod_id}.html"
                rsp = self.fetch(detail_url, headers=self.headers)
                content = rsp.text
                doc = self.html(content)

                # è·å–åŸºæœ¬ä¿¡æ¯
                title_elem = doc.xpath('//h1[@class="slide-info-title"]/text() | //h1/text()')
                title = title_elem[0].strip() if title_elem else ''

                pic_elem = doc.xpath('//div[@class="slide-info-pic"]//img/@src | //img[@class="lazy"]/@data-src')
                pic = pic_elem[0] if pic_elem else ''
                if pic and not pic.startswith('http'):
                    pic = self.host + pic

                # è·å–æè¿°ä¿¡æ¯
                desc_elem = doc.xpath('//div[@class="slide-info-desc"]/text() | //div[@class="desc"]/text()')
                desc = desc_elem[0].strip() if desc_elem else ''

                # è·å–æ¼”å‘˜ã€å¯¼æ¼”ç­‰ä¿¡æ¯
                info_texts = doc.xpath('//div[@class="slide-info"]//text()')
                director = ''
                actor = ''
                for text in info_texts:
                    if 'å¯¼æ¼”ï¼š' in text:
                        director = text.replace('å¯¼æ¼”ï¼š', '').strip()
                    elif 'ä¸»æ¼”ï¼š' in text:
                        actor = text.replace('ä¸»æ¼”ï¼š', '').strip()

                # è·å–æ’­æ”¾åˆ—è¡¨
                play_from, play_url = self._parse_play_sources(doc)

                video_info = {
                    'vod_id': vod_id,
                    'vod_name': title,
                    'vod_pic': pic,
                    'vod_remarks': '',
                    'vod_year': '',
                    'vod_area': '',
                    'vod_director': director,
                    'vod_actor': actor,
                    'vod_content': desc,
                    'vod_play_from': '$$$'.join(play_from),
                    'vod_play_url': '$$$'.join(play_url)
                }
                videos.append(video_info)

            result['list'] = videos

        except Exception as e:
            self.log(f"è·å–è¯¦æƒ…å‡ºé”™: {str(e)}")
            result['list'] = []

        return result

    def _extract_video_url(self, content):
        """æå–è§†é¢‘URL"""
        # å¸¸è§çš„è§†é¢‘URLæ¨¡å¼
        patterns = [
            r'"url":\s*"([^"]*\.m3u8[^"]*)"',
            r'"url":\s*"([^"]*\.mp4[^"]*)"',
            r'src\s*=\s*["\']([^"\']*\.m3u8[^"\']*)["\']',
            r'src\s*=\s*["\']([^"\']*\.mp4[^"\']*)["\']',
            r'video_url\s*=\s*["\']([^"\']*)["\']',
            r'playurl\s*=\s*["\']([^"\']*)["\']'
        ]

        for pattern in patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            for match in matches:
                if match and ('m3u8' in match or 'mp4' in match):
                    # è§£ç URL
                    try:
                        decoded_url = urllib.parse.unquote(match)
                        if decoded_url.startswith('http'):
                            return decoded_url
                    except:
                        if match.startswith('http'):
                            return match
        return ''

    def playerContent(self, flag, id, vipFlags):
        """è·å–æ’­æ”¾é“¾æ¥"""
        result = {}

        try:
            # è§£ææ’­æ”¾æºSID
            expected_sid = None
            if flag and '[SID:' in flag and ']' in flag:
                sid_match = re.search(r'\[SID:(\d+)\]', flag)
                if sid_match:
                    expected_sid = sid_match.group(1)

            play_url = self.host + id if id.startswith('/index.php/vod/play/') else self.host + id
            rsp = self.fetch(play_url, headers=self.headers)
            content = rsp.text

            # æŸ¥æ‰¾iframeä¸­çš„è§†é¢‘é“¾æ¥
            iframe_pattern = r'<iframe[^>]*src=["\']([^"\']*)["\'][^>]*>'
            iframe_matches = re.findall(iframe_pattern, content, re.IGNORECASE)

            for iframe_url in iframe_matches:
                if iframe_url and 'player' in iframe_url.lower():
                    if not iframe_url.startswith('http'):
                        iframe_url = urllib.parse.urljoin(self.host, iframe_url)

                    iframe_rsp = self.fetch(iframe_url, headers=self.headers)
                    iframe_content = iframe_rsp.text

                    video_url = self._extract_video_url(iframe_content)
                    if video_url:
                        result['parse'] = 0
                        result['playUrl'] = ''
                        result['url'] = video_url
                        result['header'] = {}
                        return result

            # ç›´æ¥åœ¨é¡µé¢ä¸­æŸ¥æ‰¾è§†é¢‘URL
            video_url = self._extract_video_url(content)
            if video_url:
                result['parse'] = 0
                result['playUrl'] = ''
                result['url'] = video_url
                result['header'] = {}
                return result

        except Exception as e:
            self.log(f"è·å–æ’­æ”¾é“¾æ¥å‡ºé”™: {str(e)}")

        return result

    def isVideoFormat(self, url):
        """åˆ¤æ–­æ˜¯å¦ä¸ºè§†é¢‘æ ¼å¼"""
        pass

    def manualVideoCheck(self):
        """æ‰‹åŠ¨è§†é¢‘æ£€æŸ¥"""
        pass

    def destroy(self):
        """é”€æ¯"""
        pass

    def localProxy(self, param):
        """æœ¬åœ°ä»£ç†"""
        return None
